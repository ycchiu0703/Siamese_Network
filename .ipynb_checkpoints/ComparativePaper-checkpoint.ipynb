{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11166e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152440/2594393937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "RMSProp = keras.optimizers.RMSprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = np.load('./img_label.npy',allow_pickle=True)\n",
    "# dataset = np.load('./img_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_fam_cont = {'mirai': 0, 'unknown': 0, 'penguin': 0, 'fakebank': 0, 'fakeinst': 0, 'mobidash': 0, 'berbew': 0, 'wroba': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = list(label)\n",
    "# dataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7416a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10 each for 'mirai','unknown','penguin','fakebank', 'fakeinst', 'mobidash', 'berbew', 'wroba' \n",
    "# sub_dataset = []\n",
    "# sub_label = []\n",
    "# for i in range(len(label)):\n",
    "#     if mal_fam_cont[label[i]] < 10:\n",
    "#         sub_dataset.append(dataset[i])\n",
    "#         sub_label.append(label[i])\n",
    "#         mal_fam_cont[label[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mal_fam_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'./Labels_TimeSyscallSeqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# drop_fam = ['metasploit','zbot','skeeyah','hiddad']\n",
    "drop_index = []\n",
    "dataset = dataset.drop(['CLASS', 'BEH'], axis=1)\n",
    "for i in range(len(dataset)):\n",
    "    if dataset['FAM'][i] in mal_fam_cont and mal_fam_cont[dataset['FAM'][i]] < 10:\n",
    "        mal_fam_cont[dataset['FAM'][i]] += 1\n",
    "    else:\n",
    "        drop_index.append(i)\n",
    "        \n",
    "dataset = dataset.drop(drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_fam_label = {'mirai': 0, 'unknown': 1, 'penguin': 2, 'fakebank': 3, 'fakeinst': 4, 'mobidash': 5, 'berbew': 6, 'wroba': 7}\n",
    "label = []\n",
    "for fam in dataset.FAM:\n",
    "    label.append(mal_fam_label[fam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSyscallSeq(SeqStr):\n",
    "    SeqStr = ast.literal_eval(SeqStr)\n",
    "    return [syscall for time, syscall in SeqStr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq = list()\n",
    "Seq = [ ExtractSyscallSeq(seq) for seq in dataset.SEQUENCE ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram word2vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram_transformer = Phrases(Seq)\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 #40\n",
    "\n",
    "model = Word2Vec(bigram_transformer[Seq], min_count = 1)\n",
    "word_dict = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(Seq)\n",
    "X = tokenizer.texts_to_sequences(Seq)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# padding, cuz the len of each sample is different\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#check how many words we use\n",
    "num_words = min(MAX_NB_WORDS, len(word_dict) + 1) \n",
    "\n",
    "# set embedding matrix\n",
    "\n",
    "num_words = len(word_dict)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for i,w_v in enumerate(word_dict.items()):\n",
    "    embedding_matrix[i + 1] = w_v[1]\n",
    "\n",
    "# matrix -> layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(model.wv.vectors)\n",
    "# weights=[embedding_matrix]\n",
    "embedding = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f562758",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# # Load word2vec pre-train model\n",
    "# model = gensim.models.Word2Vec.load('./word2vec_pretrain_v300.model')\n",
    "# weights = torch.FloatTensor(model.wv.vectors)\n",
    "\n",
    "\n",
    "# # Build nn.Embedding() layer\n",
    "# embedding = nn.Embedding.from_pretrained(weights)\n",
    "# embedding.requires_grad = False\n",
    "\n",
    "\n",
    "# # Query\n",
    "# query = '天氣'\n",
    "# query_id = torch.tensor(model.wv.vocab['天氣'].index)\n",
    "\n",
    "# gensim_vector = torch.tensor(model[query])\n",
    "# embedding_vector = embedding(query_id)\n",
    "\n",
    "# print(gensim_vector==embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_dict['sendto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676753f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the support set\n",
    "\n",
    "support_set = np.empty([4,100], dtype=np.int_)\n",
    "count=0\n",
    "for i in range(40):\n",
    "    if i%10==0:\n",
    "        support_set[count]=X[i]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28327e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = { 0: 0, 1: 0, 2: 0, 3: 0, 4 : 0, 5: 0, 6: 0, 7: 0}\n",
    "for i in Y_train:\n",
    "    num[i] += 1\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # 建立 XGBClassifier 模型\n",
    "# xgboostModel = XGBClassifier(n_estimators=100, learning_rate= 0.5)\n",
    "# # 使用訓練資料訓練模型\n",
    "# xgboostModel.fit(X_train, Y_train)\n",
    "# # 使用訓練資料預測分類\n",
    "# predicted = xgboostModel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 預測成功的比例\n",
    "# print('訓練集: ',xgboostModel.score(X_train,Y_train))\n",
    "# print('測試集: ',xgboostModel.score(X_test,Y_test))\n",
    "# 訓練集:  1.0\n",
    "# 測試集:  0.20833333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f086745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def the func -> make pair\n",
    "# make pair\n",
    "import random\n",
    "def make_pairs(x, y):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(5):\n",
    "        for idx1 in range(len(x)):\n",
    "            x1 = x[idx1]\n",
    "            label1 = int(y[idx1])\n",
    "            idx2=random.randint(0,len(x)-1)\n",
    "            while(idx1 == idx2 or int(y[idx2])==label1):\n",
    "                idx2=random.randint(0,len(x)-1)\n",
    "            x2 = x[idx2]\n",
    "            pairs += [[x1, x2]]\n",
    "            labels +=[0]\n",
    "            idx2=random.randint(0,len(x)-1)\n",
    "            while(idx1 == idx2 or int(y[idx2])!=label1):\n",
    "                idx2=random.randint(0,len(x)-1)\n",
    "            x2 = x[idx2]\n",
    "            pairs += [[x1, x2]]\n",
    "            labels +=[1]\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, Y_train)\n",
    "x_train_1 = pairs_train[:, 0] \n",
    "x_train_2 = pairs_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def siamese network\n",
    "\n",
    "def siamese_network(input_shape):\n",
    "    image1 = keras.layers.Input(input_shape)\n",
    "    image2 = keras.layers.Input(input_shape)  \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(units=64,\n",
    "                            return_sequences=True),\n",
    "            input_shape=(MAX_SEQUENCE_LENGTH , 1)\n",
    "        )\n",
    "    )\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=8,kernel_size = 3, padding=\"same\", activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=3 ,  padding='same')) \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=50,activation='relu'))\n",
    "    model.add(Dense(units=10,activation='relu'))\n",
    "    #model.add(Dense(units=count,activation='relu'))\n",
    "    model.summary()\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    encoded_1 = model(image1)\n",
    "    encoded_2 = model(image2)   \n",
    "    lambda_result = keras.layers.Lambda(lambda x: K.sqrt(K.maximum(K.sum(K.square(x[0] - x[1]), axis = 1, keepdims=True), K.epsilon())))([encoded_1, encoded_2])\n",
    "    return keras.Model(inputs=[image1,image2],outputs=lambda_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def some function and model\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    sqaure_pred =  K.square(y_pred)\n",
    "    margin_square =  K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "input_shape= x_train_1.shape[1:]\n",
    "train_siamese = siamese_network(input_shape)    \n",
    "train_siamese.compile(optimizer = RMSProp(), loss = contrastive_loss, metrics=[accuracy],)\n",
    "history=train_siamese.fit([x_train_1,x_train_2],\n",
    "                labels_train,\n",
    "                batch_size= batch_size,\n",
    "                epochs= epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing, metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import dump,load\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):   # 继承 torch 的 Module\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()    # \n",
    "        # 初始化三层神经网络 两个全连接的隐藏层，一个输出层\n",
    "        self.emb = torch.nn.Embedding(num_embeddings = 62, embedding_dim = 100,padding_idx = None)\n",
    "        self.fc1 = torch.nn.Linear(100,128)  # 第一个隐含层  \n",
    "        self.fc2 = torch.nn.Linear(128,256)  # 第二个隐含层\n",
    "        self.fc3 = torch.nn.Linear(256,64)\n",
    "        self.fc4 = torch.nn.Linear(64,16)\n",
    "        # self.fc5= torch.nn.Linear(256,256)\n",
    "        # self.fc6 = torch.nn.Linear(256,64)\n",
    "        self.fc5= torch.nn.Linear(16,8)   # 输出层\n",
    "        \n",
    "    def forward(self,din):\n",
    "        # 前向传播， 输入值：din, 返回值 dout\n",
    "        # din = din.view(-1,28*28)       # 将一个多行的Tensor,拼接成一行\n",
    "        dout = self.emb(din)\n",
    "        dout = F.relu(self.fc1(din))   # 使用 relu 激活函数\n",
    "        dout = F.relu(self.fc2(dout))\n",
    "        dout = F.relu(self.fc3(dout))\n",
    "        dout = F.relu(self.fc4(dout))\n",
    "        # dout = F.relu(self.fc5(dout))\n",
    "        # dout = F.relu(self.fc6(dout))\n",
    "        dout = F.softmax(self.fc5(dout), dim = 1)  # 输出层使用 softmax 激活函数\n",
    "        # 10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2040d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_init = np.array(X_train)\n",
    "train_label_init = np.array(Y_train)\n",
    "train_features_init = torch.from_numpy(train_features_init)\n",
    "train_label_init = np.array(train_label_init).astype('float32') # 轉換成 nparray\n",
    "train_label_init = torch.from_numpy(train_label_init).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "strace_train = torch.utils.data.TensorDataset(train_features_init,train_label_init)\n",
    "\n",
    "#Testdata\n",
    "test_features_init = np.array(X_test)\n",
    "test_label_init = np.array(Y_test)\n",
    "test_features_init = torch.from_numpy(test_features_init)\n",
    "test_label_init = np.array(test_label_init).astype('float32') # 轉換成 nparray\n",
    "test_label_init = torch.from_numpy(test_label_init).type(torch.LongTensor)\n",
    "\n",
    "strace_test = torch.utils.data.TensorDataset(test_features_init, test_label_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed21ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30     # epoch 的数目\n",
    "batch_size = 1  # 决定每次读取多少图10\n",
    "train_loader = torch.utils.data.DataLoader(strace_train, batch_size = batch_size, num_workers = 0)\n",
    "test_loader = torch.utils.data.DataLoader(strace_test, batch_size = batch_size, num_workers = 0)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 训练集中不需要反向传播\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa494ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for data,target in train_loader:\n",
    "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "            output = model(data.float())    # 得到预测值\n",
    "            loss = lossfunc(output,target)  # 计算两者的误差\n",
    "            loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "            optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "        # 每遍历一遍数据集，测试一下准确率\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7590892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ycc1",
   "language": "python",
   "name": "ycc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
