{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11166e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 19:54:18.896366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-17 19:54:19.070558: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-17 19:54:19.074599: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:54:19.074613: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-17 19:54:19.098038: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-17 19:54:19.631908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:54:19.631979: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:54:19.631983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "RMSProp = keras.optimizers.RMSprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a709c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = np.load('./img_label.npy',allow_pickle=True)\n",
    "# dataset = np.load('./img_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba29cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_fam_cont = {'mirai': 0, 'unknown': 0, 'penguin': 0, 'fakebank': 0, 'fakeinst': 0, 'mobidash': 0, 'berbew': 0, 'wroba': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb3cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = list(label)\n",
    "# dataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7416a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10 each for 'mirai','unknown','penguin','fakebank', 'fakeinst', 'mobidash', 'berbew', 'wroba' \n",
    "# sub_dataset = []\n",
    "# sub_label = []\n",
    "# for i in range(len(label)):\n",
    "#     if mal_fam_cont[label[i]] < 10:\n",
    "#         sub_dataset.append(dataset[i])\n",
    "#         sub_label.append(label[i])\n",
    "#         mal_fam_cont[label[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6e0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mal_fam_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d5116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'./../Labels_TimeSyscallSeqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6de3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# drop_fam = ['metasploit','zbot','skeeyah','hiddad']\n",
    "drop_index = []\n",
    "dataset = dataset.drop(['CLASS', 'BEH'], axis=1)\n",
    "for i in range(len(dataset)):\n",
    "    if dataset['FAM'][i] in mal_fam_cont and mal_fam_cont[dataset['FAM'][i]] < 10:\n",
    "        mal_fam_cont[dataset['FAM'][i]] += 1\n",
    "    else:\n",
    "        drop_index.append(i)\n",
    "        \n",
    "dataset = dataset.drop(drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66b1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f922a7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>FAM</th>\n",
       "      <th>SEQUENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca970cc75ccdf7bd1959b970de1f81d682ea3af871aff6...</td>\n",
       "      <td>mirai</td>\n",
       "      <td>[('1659223215.715424', 'execve'), ('1659223215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>699ffec8df22106524defd950bff835735051514a9b059...</td>\n",
       "      <td>mirai</td>\n",
       "      <td>[('1662079445.156731', 'execve'), ('1662079445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94e883f37be5e6a2791b081e78c86804b427e90e55a172...</td>\n",
       "      <td>mirai</td>\n",
       "      <td>[('946684902.916742', 'execve'), ('946684903.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6b8490cc0ea6b234de8b6529b9277ee0ccba22026b4689...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[('1970.5854504280915968', 'wait4'), ('1970.62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93a9669eea9b49258a1c4ca20b261561b6bd91b43643ad...</td>\n",
       "      <td>mirai</td>\n",
       "      <td>[('1970.-1293549660274688', 'read'), ('1970.-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>94a85f288f8e88b821b61bcb1caa08c1f38230c9da0a11...</td>\n",
       "      <td>penguin</td>\n",
       "      <td>[('1970.000000', 'execve'), ('2022.064030', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>814e3aa1aec837c9d9e4e2ae03f66cd821fb648237cf69...</td>\n",
       "      <td>penguin</td>\n",
       "      <td>[('1970.000000', 'execve'), ('2022.052163', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>334a894d9e7e7d8b3869212e5a77b2a897b6a7171b5f4e...</td>\n",
       "      <td>penguin</td>\n",
       "      <td>[('1970.000000', 'execve'), ('2022.051174', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>b468d1f8084fbefa850888d87e42ad5f30079547b66823...</td>\n",
       "      <td>penguin</td>\n",
       "      <td>[('1970.000000', 'execve'), ('2022.068917', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>f44b3318e48154c94ae83d33bae607c744c88250991cb7...</td>\n",
       "      <td>penguin</td>\n",
       "      <td>[('1970.000000', 'execve'), ('2022.017995', 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             FILENAME      FAM  \\\n",
       "0   ca970cc75ccdf7bd1959b970de1f81d682ea3af871aff6...    mirai   \n",
       "1   699ffec8df22106524defd950bff835735051514a9b059...    mirai   \n",
       "2   94e883f37be5e6a2791b081e78c86804b427e90e55a172...    mirai   \n",
       "3   6b8490cc0ea6b234de8b6529b9277ee0ccba22026b4689...  unknown   \n",
       "4   93a9669eea9b49258a1c4ca20b261561b6bd91b43643ad...    mirai   \n",
       "..                                                ...      ...   \n",
       "75  94a85f288f8e88b821b61bcb1caa08c1f38230c9da0a11...  penguin   \n",
       "76  814e3aa1aec837c9d9e4e2ae03f66cd821fb648237cf69...  penguin   \n",
       "77  334a894d9e7e7d8b3869212e5a77b2a897b6a7171b5f4e...  penguin   \n",
       "78  b468d1f8084fbefa850888d87e42ad5f30079547b66823...  penguin   \n",
       "79  f44b3318e48154c94ae83d33bae607c744c88250991cb7...  penguin   \n",
       "\n",
       "                                             SEQUENCE  \n",
       "0   [('1659223215.715424', 'execve'), ('1659223215...  \n",
       "1   [('1662079445.156731', 'execve'), ('1662079445...  \n",
       "2   [('946684902.916742', 'execve'), ('946684903.7...  \n",
       "3   [('1970.5854504280915968', 'wait4'), ('1970.62...  \n",
       "4   [('1970.-1293549660274688', 'read'), ('1970.-1...  \n",
       "..                                                ...  \n",
       "75  [('1970.000000', 'execve'), ('2022.064030', 'c...  \n",
       "76  [('1970.000000', 'execve'), ('2022.052163', 'b...  \n",
       "77  [('1970.000000', 'execve'), ('2022.051174', 'n...  \n",
       "78  [('1970.000000', 'execve'), ('2022.068917', 'b...  \n",
       "79  [('1970.000000', 'execve'), ('2022.017995', 'e...  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fe4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_fam_label = {'mirai': 0, 'unknown': 1, 'penguin': 2, 'fakebank': 3, 'fakeinst': 4, 'mobidash': 5, 'berbew': 6, 'wroba': 7}\n",
    "label = []\n",
    "for fam in dataset.FAM:\n",
    "    label.append(mal_fam_label[fam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dece126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSyscallSeq(SeqStr):\n",
    "    SeqStr = ast.literal_eval(SeqStr)\n",
    "    return [syscall for time, syscall in SeqStr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cdeb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq = list()\n",
    "Seq = [ ExtractSyscallSeq(seq) for seq in dataset.SEQUENCE ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "080a4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in range(80):\n",
    "    max_len = max(max_len, len(Seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ec6849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5332"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram word2vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram_transformer = Phrases(Seq)\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 #40\n",
    "\n",
    "model = Word2Vec(bigram_transformer[Seq], min_count = 1)\n",
    "word_dict = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(Seq)\n",
    "X = tokenizer.texts_to_sequences(Seq)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# padding, cuz the len of each sample is different\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#check how many words we use\n",
    "num_words = min(MAX_NB_WORDS, len(word_dict) + 1) \n",
    "\n",
    "# set embedding matrix\n",
    "\n",
    "num_words = len(word_dict)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for i,w_v in enumerate(word_dict.items()):\n",
    "    embedding_matrix[i + 1] = w_v[1]\n",
    "\n",
    "# matrix -> layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0c478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.FloatTensor(model.wv.vectors)\n",
    "# # weights=[embedding_matrix]\n",
    "# embedding = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f562758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8198115c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5c2378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "676753f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the support set\n",
    "\n",
    "support_set = np.empty([4,100], dtype=np.int_)\n",
    "count=0\n",
    "for i in range(40):\n",
    "    if i%10==0:\n",
    "        support_set[count]=X[i]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3aa7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5400483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28327e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 8, 1: 9, 2: 6, 3: 5, 4: 7, 5: 7, 6: 7, 7: 7}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = { 0: 0, 1: 0, 2: 0, 3: 0, 4 : 0, 5: 0, 6: 0, 7: 0}\n",
    "for i in Y_train:\n",
    "    num[i] += 1\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ff0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # 建立 XGBClassifier 模型\n",
    "# xgboostModel = XGBClassifier(n_estimators=100, learning_rate= 0.5)\n",
    "# # 使用訓練資料訓練模型\n",
    "# xgboostModel.fit(X_train, Y_train)\n",
    "# # 使用訓練資料預測分類\n",
    "# predicted = xgboostModel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5cf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 預測成功的比例\n",
    "# print('訓練集: ',xgboostModel.score(X_train,Y_train))\n",
    "# print('測試集: ',xgboostModel.score(X_test,Y_test))\n",
    "# 訓練集:  1.0\n",
    "# 測試集:  0.20833333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f086745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def the func -> make pair\n",
    "# make pair\n",
    "import random\n",
    "def make_pairs(x, y):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(5):\n",
    "        for idx1 in range(len(x)):\n",
    "            x1 = x[idx1]\n",
    "            label1 = int(y[idx1])\n",
    "            idx2=random.randint(0,len(x)-1)\n",
    "            while(idx1 == idx2 or int(y[idx2])==label1):\n",
    "                idx2=random.randint(0,len(x)-1)\n",
    "            x2 = x[idx2]\n",
    "            pairs += [[x1, x2]]\n",
    "            labels +=[0]\n",
    "            idx2=random.randint(0,len(x)-1)\n",
    "            while(idx1 == idx2 or int(y[idx2])!=label1):\n",
    "                idx2=random.randint(0,len(x)-1)\n",
    "            x2 = x[idx2]\n",
    "            pairs += [[x1, x2]]\n",
    "            labels +=[1]\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, Y_train)\n",
    "x_train_1 = pairs_train[:, 0] \n",
    "x_train_2 = pairs_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "979e2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def siamese network\n",
    "\n",
    "def siamese_network(input_shape):\n",
    "    image1 = keras.layers.Input(input_shape)\n",
    "    image2 = keras.layers.Input(input_shape)  \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(units=64,\n",
    "                            return_sequences=True),\n",
    "            input_shape=(MAX_SEQUENCE_LENGTH , 1)\n",
    "        )\n",
    "    )\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=8,kernel_size = 3, padding=\"same\", activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=3 ,  padding='same')) \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=50,activation='relu'))\n",
    "    model.add(Dense(units=10,activation='relu'))\n",
    "    #model.add(Dense(units=count,activation='relu'))\n",
    "    model.summary()\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    encoded_1 = model(image1)\n",
    "    encoded_2 = model(image2)   \n",
    "    lambda_result = keras.layers.Lambda(lambda x: K.sqrt(K.maximum(K.sum(K.square(x[0] - x[1]), axis = 1, keepdims=True), K.epsilon())))([encoded_1, encoded_2])\n",
    "    return keras.Model(inputs=[image1,image2],outputs=lambda_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7155bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 19:54:31.085141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-17 19:54:31.085168: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-17 19:54:31.085181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (islab-Reborn1114): /proc/driver/nvidia/version does not exist\n",
      "2022-11-17 19:54:31.085399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          6300      \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 8)            3080      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                40050     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,420\n",
      "Trainable params: 128,120\n",
      "Non-trainable params: 6,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 9s 21ms/step - loss: 0.2105 - accuracy: 0.7393\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.1359 - accuracy: 0.8161\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.1137 - accuracy: 0.8554\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 6s 20ms/step - loss: 0.1122 - accuracy: 0.8714\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0861 - accuracy: 0.9000\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0810 - accuracy: 0.9089\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0629 - accuracy: 0.9321\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0586 - accuracy: 0.9339\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0571 - accuracy: 0.9304\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.0467 - accuracy: 0.9482\n"
     ]
    }
   ],
   "source": [
    "# def some function and model\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    sqaure_pred =  K.square(y_pred)\n",
    "    margin_square =  K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "input_shape= x_train_1.shape[1:]\n",
    "train_siamese = siamese_network(input_shape)    \n",
    "train_siamese.compile(optimizer = RMSProp(), loss = contrastive_loss, metrics=[accuracy],)\n",
    "history=train_siamese.fit([x_train_1,x_train_2],\n",
    "                labels_train,\n",
    "                batch_size= batch_size,\n",
    "                epochs= epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3665f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c46b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(num_embeddings = 62, embedding_dim = 100, padding_idx = 0)\n",
    "# print(embedding.weight, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing, metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import dump,load\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44b687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):   # 继承 torch 的 Module\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()    # \n",
    "        # 初始化三层神经网络 两个全连接的隐藏层，一个输出层\n",
    "        # self.emb = torch.nn.Embedding(num_embeddings = 62, embedding_dim = 100,padding_idx = 0)\n",
    "        self.fc1 = torch.nn.Linear(100,128)  # 第一个隐含层  \n",
    "        self.fc2 = torch.nn.Linear(128,256)  # 第二个隐含层\n",
    "        self.fc3 = torch.nn.Linear(256,64)\n",
    "        self.fc4 = torch.nn.Linear(64,16)\n",
    "        # self.fc5= torch.nn.Linear(256,256)\n",
    "        # self.fc6 = torch.nn.Linear(256,64)\n",
    "        self.fc5= torch.nn.Linear(16,8)   # 输出层\n",
    "        \n",
    "    def forward(self,din):\n",
    "        # 前向传播， 输入值：din, 返回值 dout\n",
    "        # din = din.view(-1,28*28)       # 将一个多行的Tensor,拼接成一行\n",
    "        # dout = self.emb(din)\n",
    "        dout = F.relu(self.fc1(din))   # 使用 relu 激活函数\n",
    "        dout = F.relu(self.fc2(dout))\n",
    "        dout = F.relu(self.fc3(dout))\n",
    "        dout = F.relu(self.fc4(dout))\n",
    "        # dout = F.relu(self.fc5(dout))\n",
    "        # dout = F.relu(self.fc6(dout))\n",
    "        dout = F.softmax(self.fc5(dout), dim = 1)  # 输出层使用 softmax 激活函数\n",
    "        # 10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b2040d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_init = np.array(X_train)\n",
    "train_label_init = np.array(Y_train)\n",
    "train_features_init = torch.from_numpy(train_features_init)\n",
    "train_label_init = np.array(train_label_init).astype('float32') # 轉換成 nparray\n",
    "train_label_init = torch.from_numpy(train_label_init).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "strace_train = torch.utils.data.TensorDataset(train_features_init,train_label_init)\n",
    "\n",
    "#Testdata\n",
    "test_features_init = np.array(X_test)\n",
    "test_label_init = np.array(Y_test)\n",
    "test_features_init = torch.from_numpy(test_features_init)\n",
    "test_label_init = np.array(test_label_init).astype('float32') # 轉換成 nparray\n",
    "test_label_init = torch.from_numpy(test_label_init).type(torch.LongTensor)\n",
    "\n",
    "strace_test = torch.utils.data.TensorDataset(test_features_init, test_label_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bed21ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30     # epoch 的数目\n",
    "batch_size = 1  # 决定每次读取多少图10\n",
    "train_loader = torch.utils.data.DataLoader(strace_train, batch_size = batch_size, num_workers = 0)\n",
    "test_loader = torch.utils.data.DataLoader(strace_test, batch_size = batch_size, num_workers = 0)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07fb0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 训练集中不需要反向传播\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aa494ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for data,target in train_loader:\n",
    "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "            output = model(data.float())    # 得到预测值\n",
    "            loss = lossfunc(output,target)  # 计算两者的误差\n",
    "            loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "            optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "        # 每遍历一遍数据集，测试一下准确率\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7590892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1932c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  \tTraining Loss: 2.087791\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  2  \tTraining Loss: 2.054752\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  3  \tTraining Loss: 1.983831\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  4  \tTraining Loss: 1.904745\n",
      "Accuracy of the network on the test images: 25 %\n",
      "Epoch:  5  \tTraining Loss: 1.836408\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  6  \tTraining Loss: 1.764550\n",
      "Accuracy of the network on the test images: 29 %\n",
      "Epoch:  7  \tTraining Loss: 1.742721\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  8  \tTraining Loss: 1.797265\n",
      "Accuracy of the network on the test images: 25 %\n",
      "Epoch:  9  \tTraining Loss: 1.893600\n",
      "Accuracy of the network on the test images: 12 %\n",
      "Epoch:  10  \tTraining Loss: 1.792551\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  11  \tTraining Loss: 1.671749\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  12  \tTraining Loss: 1.666242\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  13  \tTraining Loss: 1.696645\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  14  \tTraining Loss: 1.605420\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  15  \tTraining Loss: 1.548577\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  16  \tTraining Loss: 1.533176\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  17  \tTraining Loss: 1.546360\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  18  \tTraining Loss: 1.565971\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  19  \tTraining Loss: 1.563213\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  20  \tTraining Loss: 1.691947\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  21  \tTraining Loss: 1.495740\n",
      "Accuracy of the network on the test images: 12 %\n",
      "Epoch:  22  \tTraining Loss: 1.834317\n",
      "Accuracy of the network on the test images: 12 %\n",
      "Epoch:  23  \tTraining Loss: 1.687329\n",
      "Accuracy of the network on the test images: 29 %\n",
      "Epoch:  24  \tTraining Loss: 1.513533\n",
      "Accuracy of the network on the test images: 20 %\n",
      "Epoch:  25  \tTraining Loss: 1.408937\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  26  \tTraining Loss: 1.464286\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  27  \tTraining Loss: 1.450221\n",
      "Accuracy of the network on the test images: 25 %\n",
      "Epoch:  28  \tTraining Loss: 1.460842\n",
      "Accuracy of the network on the test images: 16 %\n",
      "Epoch:  29  \tTraining Loss: 1.359632\n",
      "Accuracy of the network on the test images: 25 %\n",
      "Epoch:  30  \tTraining Loss: 1.363289\n",
      "Accuracy of the network on the test images: 16 %\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea7ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ycc1",
   "language": "python",
   "name": "ycc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
